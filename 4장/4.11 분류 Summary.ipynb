{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2ef8a3",
   "metadata": {},
   "source": [
    "# 4장 분류(Classification) Sumary\n",
    "\n",
    "> 결정 트리 알고리즘\n",
    "- 정보의 균일도에 기반한 규칙 트리를 만들어 예측 수행\n",
    "- 장점 : 비교적 직관적이어서 어떻게 예측 결과 도출되었는지 알기 쉬움\n",
    "- 단점 : 균일한 최종 예측 결과를 도출하기 위해 결정 트리가 깊어지고 복잡해지면서 과적합 발생이 쉬움\n",
    "\n",
    "> 앙상블 기법\n",
    "- 대부분의 결정 트리 기반의 다수의 약한 학습기(Weak Learner)를 결합해 변동성을 줄여 예측 오류를 줄이고 성능 개선\n",
    "\n",
    ">> 앙상블 기법의 종류\n",
    ">> 1) 배깅(Bagging)\n",
    ">> - 학습데이터를 중복 허용하면서 다수의 세트로 샘플링하여 이를 다수의 약한 학습기가 학습한 뒤 최종 결과를 결합해 예측\n",
    ">> - 대표적인 배깅 방식은 랜덤 포레스트로 수행시간이 빠르고 비교적 안정적인 예측 성능 제공\n",
    "\n",
    ">> 2) 부스팅(Boosting)\n",
    ">> - 순차적 학습 진행하면서 예측 틀린 데이터에 대해 가중치를 부여해, 다음번 학습기가 학습할 때 이전 예측이 틀린 데이터에 대해서는 보다 높은 정확도로 예측할 수 있도록 함\n",
    ">> - 대표적인 방식은 GBM(Gradient Boosting Mahcine)으로 뛰어난 예측 성능을 가졌지만 수행시간이 너무 오래 걸림\n",
    ">> - XGBoost(eXtra Gradient Boost)는 GBM 기반이지만, GBM의 단점인 느린 수행시간과 과적합 규제 등을 해결한 알고리즘\n",
    ">> - LightGBM은 XGBoost 대비 더 빠른 학습/예측 수행 시간과 작은 메모리 사용량, 카테고리형 피처의 자동변환 및 최적분할 수행 해주지만 적은 데이터셋에 적용시 과적합 발생이 쉬움(10,000건 이하 데이터 셋)\n",
    "\n",
    "> 스태킹(Stacking) 기법\n",
    "- 여러 개의 개별 모델들이 생성한 예측 데이터 셋 기반 최종 메타 모델이 학습할 별도의 학습/예측 데이터 셋을 재생성\n",
    "- 핵심은 메타 모델이 사용할 학습/예측 데이터 셋을 개별 모델의 예측값들을 스태킹 형태로 결합해 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f93fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
